This directory contains examples of client code that uses kafka.

To run the demo:

   1. Start Zookeeper and the Kafka server
   2. For unlimited sync-producer-consumer run, `run bin/java-producer-consumer-demo.sh sync`
   3. For unlimited async-producer-consumer run, `run bin/java-producer-consumer-demo.sh`
   4. For exactly once demo run, `run bin/exactly-once-demo.sh 6 3 50000`,
      this means we are starting 3 EOS instances with 6 topic partitions and 50000 pre-populated records.
   5. Some notes for exactly once demo:
      5.1. The Kafka server has to be on broker version 2.5 or higher.
      5.2. You could also use Intellij to run the example directly by configuring parameters as "Program arguments"

# 1. 配置环境，需要 gradle（6.6） 和 scala 环境，并配置环境变量，如果要启动服务，还需要配置 Zookeeper
# 2. 拉下代码到 idea，如果是官方源码，还需要先配置下 pom.xml 中的仓库地址，替换为阿里云仓库
# 3. 找到项目根目录下的 settings.grade 配置文件，添加到 gradle 工程，开始导入依赖
# 4. 依赖导入结束后，有些版本会有些包无法导入，需要手动执行执行 gradle wrapper, 生成 gradlew 执行文件，如果是 windows 环境，需要将根目录下
#   的 wrapper.gradle 文件中的最后一行移除 windows 批处理文件的配置给注释掉，不然在 windows 环境下根目录不会生成 gradlew.bat 批处理文件。
#   这是因为 Kafka 开发不是在 windows 环境，但是是可以在 windows 环境下编译的。
# 5. 在根目录下执行 ./gradlew assemble -x test 命令，重新编译下，编译成功后源码环境就搭建好了。
#    Note：1）因为 gradlew 没有配置环境变量，只能在项目根目录下执行。2）可以通过 ./gradlew tasks 查看所有可执行任务名称

# 如何保证消息的不丢失的方式
# 1. acks 设置为 -1，表示生产者发送完成消息后，只有 leader 节点和所有的 follower 节点都完成存储后，才给客户端发送响应。



● OP_ACCEPT 就绪条件：
当收到一个客户端连接请求时，该操作就绪。这是 ServerSocketChannel 上唯一有效的操作。
● OP_CONNECT 就绪条件：
	只有客户端 SocketChannel 会注册该事件，宕客户端调用 SocketChannel.connect() 时，该操作就绪。
● OP_READ 就绪条件：
该操作对客户端和服务端的 SocketChannel 都有效，当 OS 的读缓冲区中有数据可读时，该操作就绪。
● OP_WRITE 就绪条件：
	该操作对客户端和服务端的 SocketChannel 都有效，当 OS 的写缓冲区有空闲的空间时（大部分时候都有），该操作就绪。

OP_CONNECT
● 客户端调用 connect() 并注册 OP_CONNECT 事件后，连接操作就绪，但是连接就绪不代表连接成功。
● 在非阻塞模式下，如果是本地连接，连接会立刻建立，此时返回 true。其他场景下，连接是否完成需要通过 finishConnect() 方法判断。
● 在阻塞模式下，这个方法会一直阻塞直到连接建立或异常发生。

OP_ACCEPT
● 服务端见监听，并注册 OP_ACCEPT 事件后，就已经准备好接收客户端连接了。

OP_WRITE
● OP_WRITE 事件相对特殊，一般情况下，不应该注册 OP_WRITE 事件，OP_WRITE 就绪的条件为 OS 内核缓冲区有空闲空间（NIO 默认是水平触发，
OP_WRITE 事件是在 Socket 发送缓冲区中可用的字节数大于或等于其低水位标记 SO_SNDLOWAT 时发生），而写缓冲区绝大部分事件都是有空闲空间的，
所以当注册了该事件后，写操作一直就是就绪的，这样会导致Selector 处理线程会占用整个 CPU 资源。所以最佳实践是当确实有数据写入时再注册 OP_WRITE 事件，
并且在写完成以后马上取消注册。
●





select和poll都只提供了一个函数:select或者poll函数。
而epoll提供了三个函数，epoll_create,epoll_ctl和epoll_wait，epoll_create是创建一个epoll句柄；epoll_ctl是注册要监听的事件类型；epoll_wait则是等待事件的产生。

epoll除了提供select/poll那种IO事件的水平触发（Level Triggered）外，还提供了边缘触发（Edge Triggered），这就使得用户空间程序有可能缓存IO状态，减少epoll_wait/epoll_pwait的调用，提高应用程序效率。

水平触发(level-trggered)
只要文件描述符关联的读内核缓冲区非空，有数据可以读取，就一直发出可读信号进行通知，
当文件描述符关联的内核写缓冲区不满，有空间可以写入，就一直发出可写信号进行通知
LT模式支持阻塞和非阻塞两种方式。epoll默认的模式是LT。

边缘触发(edge-triggered)
当文件描述符关联的读内核缓冲区由空转化为非空的时候，则发出可读信号进行通知，
当文件描述符关联的内核写缓冲区由满转化为不满的时候，则发出可写信号进行通知
两者的区别在哪里呢？水平触发是只要读缓冲区有数据，就会一直触发可读信号，而边缘触发仅仅在空变为非空的时候通知一次，

LT(level triggered)是缺省的工作方式，并且同时支持block和no-block socket.在这种做法中，内核告诉你一个文件描述符是否就绪了，然后你可以对这个就绪的fd进行IO操作。
如果你不作任何操作，内核还是会继续通知你的，所以，这种模式编程出错误可能性要小一点。传统的select/poll都是这种模型的代表．

水平触发和边缘触发模式区别
读缓冲区刚开始是空的
读缓冲区写入2KB数据
水平触发和边缘触发模式此时都会发出可读信号
收到信号通知后，读取了1kb的数据，读缓冲区还剩余1KB数据
水平触发会再次进行通知，而边缘触发不会再进行通知
所以，边缘触发需要一次性的把缓冲区的数据读完为止，也就是一直读，直到读到EGAIN为止，EGAIN说明缓冲区已经空了，因为这一点，边缘触发需要设置文件句柄为非阻塞



epoll是一种Linux下高效的I/O多路复用机制，它支持两种不同的事件触发方式：水平触发和边缘触发。
在水平触发模式下，如果文件描述符上有数据可读或可写，epoll_wait()函数将立即返回该文件描述符，并且在下一次epoll_wait()调用之前将继续通知该文件描述符上的事件。
这意味着，如果你没有及时处理文件描述符上的事件，那么每次调用epoll_wait()都会返回相同的事件。


而在边缘触发模式下，只有当文件描述符状态发生变化时（例如，从无数据可读到有数据可读），epoll_wait()才会返回该文件描述符，并且只有在状态变化时才会再次通知该文件描述符上的事件。
这意味着你必须在epoll_wait()返回后立即处理该文件描述符上的所有事件，否则你可能会错过某些事件。




num.network.threads: 控制的 Processor 线程的个数，默认值为 3.
num.io.threads: 控制 Handler 线程的个数，默认值为 8.
queued.max_requests: 控制 RequestChannel 队列的容量，表示在网络线程停止读取新请求之前，可以排队等待I/O线程处理的最大请求个数，默认值 500.

增大 num.network.threads 能够增加处理网络io请求，但不读写具体数据，基本没有io等待。但如果过大，会带来线程切换的开销。
增大 queued.max.requests 能够缓存更多的请求，以撑过业务峰值。如果过大，会造成内存的浪费。
增大 num.io.threads 能提升线程处理能力，如果过大会代理线程切换开销影响处理能力。同时至少要等于硬盘的个数。


触发再平衡（re-balance）的条件：
  消费组成员列表发生变化，如新消费者加入或消费者退出消费组。
  订阅的主题的分区有变化。

消费者再平衡后重新分配分组，是如何保证各个消费者重新分组后消费消息的平滑过渡
  消费者会记录消费消息的偏移量，即消费进度。消费进度是维护在消费组里，当再平衡发生时，分配到新分区的消费者可以从消费组里读取该分区的消费进度，从而做到无缝迁移。



第一个消费者收到加入消费组的响应后执行分区分配，并将消费者的分配结果发送给协调者，协调者会发送同步消费组的响应给客户端，即分配给第一个消费者的分区
（因为此时只有一个消费者，所以是所有的分区），消费组的状态变为 Stable 状态。

当消费组状态为 Stable 后，第二个消费者加入消费组，此时消费组的状态会再变为 准备再平衡，并创建一个新的延迟操作对象。但是，此时这个延迟对象是无法完成的，因为
消费组中存在 awaitingJoinCallback = null 的消费者，即第一个消费者（因为消费组在发送加入消费组响应时，会将这个变量设置为 null），所以此时需要等待第一个
消费者重新加入消费组，延迟操作进入延迟缓存中。

如果第一个消费者在超时时间内重新加入了消费组，会通过延迟缓存再次执行刚刚创建的延迟操作，这次会满足条件，协调者会发送加入消费组响应给两个消费者。
如果在超时时间内第一个消费者没有发送加入消费组请求，第一个消费者的回调函数一直为 null，协调者在延迟操作超时后，强制执行延迟操作，但是这是只会
发送响应给第二个消费者。


正常情况下消费组状态变更过程
1）消费组初始状态为 Empty，当第一个消费者（主消费者）发送加入消费请求后，集群状态变更为 PreparingRebalance，并且会创建一个延迟操作等待后续更多的消费者加入消费组。
2）在指定的超时时间内如果没有新的消费者加入消费组或者等待的时间已经超过了最大值，则会强制完成延迟任务，返回加入消费组的响应给所有的消费者客户端，并将消费组的状态变更
为 CompletingRebalance，等待消费者发送同步消费组的请求。
3）消费者客户端在收到加入消费组响应后，主消费者会执行分区分配，并且会将分区的结果在发送同步消费组请求时同步给协调者，其他消费者也会发送同步消费组的请求，
区别是不会带分区分配结果，因为只有主消费者会可以执行分区分配。
4）协调者在收到消费者的同步消费组请求的处理逻辑也是不同的，通常情况下，普通消费者会先于主消费者发送 "同步消费组" 请求，因为主消费者需要执行分区分配，所以协调者如果
收到普通消费者的 "同步消费组" 请求，仅仅只是将回调函数保存到消费者元数据中，因为主消费者还没有发送  "同步消费组" 请求将分区分配结果带过来，还没有到返回  "同步消费组"
响应的时机。如果协调者收到了主消费者发送的  "同步消费组" 请求，除了保存回调函数，还需要将分区的分配结果持久化到内部主题中，完成持久化后，会执行消费组中每个消费者的回调函数，
将当前消费者的分区分配结果发送给消费者客户端，这样每个消费者就知道自己应该消费哪些分区，此时消费组的状态变更为 Stable。
在这个过程中，如果某个消费者因为一些原因发送 "同步消费组" 的请求延迟了，请求到达服务端协调者时消费组的状态已经变为 Stable 了，这样也没有关系，因为主消费者在完成持久化操作后，
会将各个消费者的分区分配结果都保存到了消费者元数据中了，此时的该消费者只需要从元数据中获取分区的分配结果并直接调用回调函数返回即可。


非主消费者发送 "加入消费组" 请求或 "同步消费组" 请求，但是消费组已经是 Stable 状态
    原因：服务端正常返回 "加入消费组" 的响应，但是可能由于网络或者消费者自身的原因没有收到这个响应。但是服务端消费组此刻的状态已经是 CompletingRebalance，它只需
等待消费者发送 "同步消费组" 的请求，协调者在收到主消费者发送的 "同步消费组" 的请求后，处理完成后，发送 "同步消费组" 的响应给各个消费者（不包括之前没有收到 "加入消费组"
请求的消费者，因为它没有发送 "同步消费组" 的请求，"同步消费组" 的请求依赖 "加入消费组" 的响应），然后消费组的状态变为 Stable。上面只针对非主消费者才能成立，对于主
消费者如果没有收到 "加入消费组" 响应，就无法执行分区分配的工作，由于协调者需要持久化分配结果并且返回 "同步消费组" 的请求只能由主消费的 "同步消费组" 的回调函数触发，
所以消费组的状态是无法变更为 Stable。
    处理：消费者在等待超时时间后没有收到 "加入消费组" 的响应，会重新发送 "加入消费组" 的请求，因为它认为没有发送成功，但是实际上该消费者的信息都已经成功保存到了消费组中，
此时再发送 "加入消费组" 的请求，因为本身的数据信息都没有发生变化，所以协调者会返回和之前相同的响应回去，如果这次消费者成功收到响应，会发送 "同步消费组" 的请求，协调者收
到请求后（此时的状态同样是 Stable），会将这个消费者分配的分区封装响应返回回去。协调者在处理 "同步消费组" 请求时，对于非主消费者只是保存回调函数，处理就结束了，并不返回
响应。对于主消费者，会将分配的结果持久化到内部主题中，然后调用各个消费者的回调函数，返回响应。


Kafka 消费者是如何避免提交未消费的偏移量（Kafka 为什么可以用拉取偏移量作为提交偏移量）
    在回答这个问题之前，首先要知道 Kafka 一次轮询所做的工作，定时提交偏移量这个工作就是在轮询中完成。在一次轮询中会判断提交任务是否超时，如果超时会立即提交偏移量。
之后发送拉取请求 -> 回调暂存拉取结果（此时没有更新偏移量）-> 拉取器调用获取记录集方法，更新订阅状态中的分区拉取偏移量，返回给客户端 -> 客户端处理返回的记录集。
整个流程可以看到，提交偏移量的操作在消费消息之后，一次轮询过程中，虽然在处理消息前偏移量已经更新，但是判断提交偏移量的操作是再此之前，如果要提交偏移量也必须等待下次
轮询，而在执行下次轮询时，本次的消息都已经处理完毕。所以下次提交的偏移量实际上是本次已经被消费完的消息，同理如果本次轮询也提交了偏移量，那提交的一定是上次更新的拉取
偏移量，并且这个位置以及之前的消息都已经被客户端消费了，所以才不会丢数据。


分区、副本、日志、日志分段

1）一个 topic 可以有多个分区，客户端以分区为最小处理单位生产或消费消息。以消费者为例，主题的分区越多，就可以启动越多的消费县城，消费者数量越多，消息的处理性能越好，延迟就越低。
2）Kafka 采用副本机制为一个分区备份多个分区，一个分区只有个主副本（Leader）和若干个备份副本（Follower）。主副本负责读写，备份副本负责向主副本拉取数据来同步，当主副本掉线后，
会从备份副本选举出一个新的主副本，继续为客户端提供读写服务。所以，每个副本会管理若干个主题的多个分区。
3）每个副本都对应一个日志，在将分区存储到底层的文件系统上，每个分区对应一个目录，分区目录下有多个日志分区（LogSegment），同一个目录下所有的日志分段都属于同一个分区。
4）每个日志分段在物理上是由一个数据文件和一个索引文件组成。数据文件存储真正的数据内容，索引文件存储的是数据文件的索引信息，方便随机读取分区时更快的访问数据文件。

总结 P315
1）一个日志由多个日志分段组成，日志管理了所有的日志分段
2）日志用 segment 保存了每个日志分段的基准偏移量到日志分段的映射关系
3）日志分段的基准偏移量时分区级别的绝对偏移量
4）日志分段中的第一条消息的绝对偏移量也等于日志分段的基准偏移量
5）每个日志分段由一个数据文件和三个索引文件组成
6）日志分段的数据文件和索引文件的文件名称以基准偏移量命名
7）数据文件保存消息的格式是：消息的绝对偏移量、消息大小、消息内容
8）索引文件保存消息偏移量和消息在数据文件中的物理位置
9）索引文件中索引条目的存储的相对偏移量，即消息的绝对偏移量 - 基准偏移量
10）索引文件是保存在内存中的，将整个索引文件加载到内存中，加快文件的读取。





客户端创建消息时会为每条消息指定偏移量，每批消息的偏移量都是从 0 开始的，消息到达服务端后，会读日志文件中最近一条消息的偏移量，然后将这些相对偏移量转换为真正的消息偏移量，
存储到日志文件中。客户端消息带相对偏移量信息是为了在服务端修改相对偏移量信息时，原有的字节缓冲区可以保持不变。如果不带这个偏移量信息，服务端在给消息生成偏移量信息时，需要
将这个偏移量保存到消息的缓冲区里，这样就会改变原有消息的缓冲区的大小，会造成不可复用的问题。

Kafka 的分区分为主副本和备份副本。P309


todo huangran
再平衡过程中，消费者客户端是否会停止心跳？如果没有停止心跳，消费者客户端如何处理心跳的响应？
服务端拉取消息流程
服务端处理 OFFSET_FETCH 和 LIST_OFFSET 流程
为什么Kafka消费者不循环实现获取消息的逻辑，而是让消费者客户端来轮询
消息批数据格式
如何处理相对偏移量 ---> 绝对偏移量



协调者在处理完成消费者的 "加入消费组" 请求后，会返回响应给消费者，消费者在收到 "加入消费组" 的响应后，应该在会话时间内及时发送 "同步消费组" 的请求给协调者，
否则协调者就会认为消费者出现了故障。协调者在处理 "同步消费组" 请求时，有多个地方调用了 "完成并调度下一次心跳" 方法。
1）状态为 CompletingRebalance，在设置成员元数据的回调方法后调用。
2）状态为 Stable，在发送 "同步消费组" 响应给消费者后调用。
3）状态为 CompletingRebalance，在收到主消费这的 "同步消费组" 请求，给每个消费者发送 "同步消费组" 响应后调用。

协调者创建完 "延迟操作" 对象后，一个重要的步骤是：当延迟操作相关的外部事件发生时，就需要通过延迟缓存尝试完成延迟操作。对于 "延迟加入消费组"，外部事件是
消费者发送了 "加入消费组" 请求，对于 "延迟心跳"，外部事件是协调者与消费者之间有网络通信。不管是协调者处理消费者发送的请求还是，还是协调者发送响应给消费者,
协调者都会完成本次延迟心跳，并开始调度下一次心跳。



协调者接受不了消费者发送的心跳来监控消费者是否存活，如果消费者没有在指定的截止时间内发送心跳，协调者认为消费者失败，将其从消费组中移除，这样消费者就需要执行
再平衡操作。
另外，协调者在处理 "加入消费组" 和 "同步消费组" 过程中，为了保证参与加入组的消费者及时响应，也会用心跳来监控消费者成员还存活。



日志压缩是一种相对于基于时间和大小日志清理更加细粒度的日志保留策略，它基于清理点（cleanPoint），将所有旧日志分段的消息复制到新的日志分段上。
日志压缩前后，日志分段中每条消息的偏移量和写入时总是保持一致，但日志分段的偏移量不再是连续的，消息保持相对有序。
日志压缩后，消息的物理位置会发生变化，会生成新的的日志分段，日志分段中的每条消息的物理位置会重新按照文件来组织。

墓碑标记：一条带键的消息，内容为 null，表示这条删除的消息所在偏移量之前的所有消息都需要删除。


更新副本偏移量的场景：更新本地日志的偏移量，更新远程的备份副本偏移量
1）追加消息到主副本的本地日志、备份副本拉取消息写到自己的本地日志，都会更新日志的偏移量。
2）主副本所在的服务端处理备份副本的拉取请求，也会更新分区中备份副本对应的偏移量。

更新副本的最高水位的场景：更新主副本的最高水位，更新备份副本的最高水位
1）主副本的最高水位取决于 ISR 中所有副本的最小偏移量。最小值没有变化，最高水位也不会变化。
2）备份副本的最高水位取决于主副本的最高水位和它自己的偏移量，它会选择两者的最小值。

副本管理器会定时将所有分区的副本最高水位，刷写到复制点文件（replication-offset-checkpoint 检查点文件）
日志管理器会定时将所有分区的副本偏移量，刷写到恢复点文件（recovery-point-offset-checkpoint 检查点文件）

延迟拉取有两种：备份副本和消费者的拉取。备份副本或消费者的拉取请求都可以有多个分区，但是服务端完成延迟拉取操作并不需要等待所有分区都收集够最少字节数，
它只需要所有分区加起来的大小满足最少字节数，就可以返回拉取结果给备份副本或消费者。
与拉取请求相反，生产者如果有多个分区，服务端完成延迟生产操作，必须等待所有分区都被 ISR 所有副本同步后，才会返回生产结果给生产者。

延迟生产和延迟拉取
  外部事件通过指定分区尝试完成的延迟操作，如果延迟操作可以完成，其他分区中的延迟操作并不会被立即删除。这是因为分区作为延迟缓存的键，在服务端数量
会很多，如果一个个检查所有分区，再从延迟缓存中删除已经完成的延迟操作，速度就很慢。另外，如果采用这种方式，只要分区对应的延迟操作完成了一个，就要
立即检查所有分区，对服务端的性能影响也较大。所以，Kafka 的延迟缓存还有一个清理器，会负责定时地清理所有分区中已经完成的延迟操作。P388_P390



分区重新分配的逻辑
1）zk 节点的数据和控制器上下文

OAR: 分区的原始副本集合（分区重分配前）
RAR: 重新分配的副本集合（分区重分配后）
AR、ISR: 分区的当前副本集以及主副本保持同步的副本集

重新分配分区过程：
1）将 RAR 加入 AR，此时 AR = OAR + RAR
2）将 RAR 加入 ISR，此时 ISR = OAR + RAR
3）ISR 不变，但是主副本从 RAR 中选举
4）将 OAR 从 ISR 中移除，此时 ISR = RAR
5）将 OAR 从 AR 中移除，此时 AR = RAR
重新分配完分区后，分区的 AR 和 ISR 都等于 RAR。增加和修改 AR 和 ISR 的顺序是： 增加 AR -> 增加 ISR -> 减少 ISR -> 减少 AR。
因为副本在 AR 中，不一定在 ISR 中，而副本在 ISR 中，一般会在 AR 中。


控制器向代理节点发送的请求类型有三种：分区的主副本和 ISR 信息（LAI）、更新元数据（UpdateMetadata）、停止副本（StopReplicas）。代理节点处理这
三种请求，都会转交给副本管理器执行具体的逻辑。由于这三种请求都会更新分区的副本状态，因此这三个操作都会使用同一个对象锁进行同步。

整体流程：
1）控制器发送 LAI 请求给分区的所有代理节点，不同代理节点的分区会分别创建分区的主副本或备份副本。主副本会将分区从拉取管理器移除，备份副本会将分区加入
拉取管理器。
2）控制器发送 UpdateMetadata 请求给集群所有代理节点。
3）每个代理节点处理 UpdateMetadata 请求，都会更新元数据缓存。
4）客户端发送 Metadata 请求，从元数据缓存中获取主题的元数据。
5）客户端从主题元数据中找出分区的主副本，并和分区的主副本进行通信。


切换主副本 日志文件怎么处理的？
代理节点下线如果处理主分区的？










