This directory contains examples of client code that uses kafka.

To run the demo:

   1. Start Zookeeper and the Kafka server
   2. For unlimited sync-producer-consumer run, `run bin/java-producer-consumer-demo.sh sync`
   3. For unlimited async-producer-consumer run, `run bin/java-producer-consumer-demo.sh`
   4. For exactly once demo run, `run bin/exactly-once-demo.sh 6 3 50000`,
      this means we are starting 3 EOS instances with 6 topic partitions and 50000 pre-populated records.
   5. Some notes for exactly once demo:
      5.1. The Kafka server has to be on broker version 2.5 or higher.
      5.2. You could also use Intellij to run the example directly by configuring parameters as "Program arguments"

# 1. 配置环境，需要 gradle（6.6） 和 scala 环境，并配置环境变量，如果要启动服务，还需要配置 Zookeeper
# 2. 拉下代码到 idea，如果是官方源码，还需要先配置下 pom.xml 中的仓库地址，替换为阿里云仓库
# 3. 找到项目根目录下的 settings.grade 配置文件，添加到 gradle 工程，开始导入依赖
# 4. 依赖导入结束后，有些版本会有些包无法导入，需要手动执行执行 gradle wrapper, 生成 gradlew 执行文件，如果是 windows 环境，需要将根目录下
#   的 wrapper.gradle 文件中的最后一行移除 windows 批处理文件的配置给注释掉，不然在 windows 环境下根目录不会生成 gradlew.bat 批处理文件。
#   这是因为 Kafka 开发不是在 windows 环境，但是是可以在 windows 环境下编译的。
# 5. 在根目录下执行 ./gradlew assemble -x test 命令，重新编译下，编译成功后源码环境就搭建好了。
#    Note：1）因为 gradlew 没有配置环境变量，只能在项目根目录下执行。2）可以通过 ./gradlew tasks 查看所有可执行任务名称

# 如何保证消息的不丢失的方式
# 1. acks 设置为 -1，表示生产者发送完成消息后，只有 leader 节点和所有的 follower 节点都完成存储后，才给客户端发送响应。

num.network.threads: 控制的 Processor 线程的个数，默认值为 3.
num.io.threads: 控制 Handler 线程的个数，默认值为 8.
queued.max_requests: 控制 RequestChannel 队列的容量，表示在网络线程停止读取新请求之前，可以排队等待I/O线程处理的最大请求个数，默认值 500.

增大 num.network.threads 能够增加处理网络io请求，但不读写具体数据，基本没有io等待。但如果过大，会带来线程切换的开销。
增大 queued.max.requests 能够缓存更多的请求，以撑过业务峰值。如果过大，会造成内存的浪费。
增大 num.io.threads 能提升线程处理能力，如果过大会代理线程切换开销影响处理能力。同时至少要等于硬盘的个数。


触发再平衡（re-balance）的条件：
  消费组成员列表发生变化，如新消费者加入或消费者退出消费组。
  订阅的主题的分区有变化。

消费者再平衡后重新分配分组，是如何保证各个消费者重新分组后消费消息的平滑过渡
  消费者会记录消费消息的偏移量，即消费进度。消费进度是维护在消费组里，当再平衡发生时，分配到新分区的消费者可以从消费组里读取该分区的消费进度，从而做到无缝迁移。



第一个消费者收到加入消费组的响应后执行分区分配，并将消费者的分配结果发送给协调者，协调者会发送同步消费组的响应给客户端，即分配给第一个消费者的分区
（因为此时只有一个消费者，所以是所有的分区），消费组的状态变为 Stable 状态。

当消费组状态为 Stable 后，第二个消费者加入消费组，此时消费组的状态会再变为 准备再平衡，并创建一个新的延迟操作对象。但是，此时这个延迟对象是无法完成的，因为
消费组中存在 awaitingJoinCallback = null 的消费者，即第一个消费者（因为消费组在发送加入消费组响应时，会将这个变量设置为 null），所以此时需要等待第一个
消费者重新加入消费组，延迟操作进入延迟缓存中。

如果第一个消费者在超时时间内重新加入了消费组，会通过延迟缓存再次执行刚刚创建的延迟操作，这次会满足条件，协调者会发送加入消费组响应给两个消费者。
如果在超时时间内第一个消费者没有发送加入消费组请求，第一个消费者的回调函数一直为 null，协调者在延迟操作超时后，强制执行延迟操作，但是这是只会
发送响应给第二个消费者。


正常情况下消费组状态变更过程
1）消费组初始状态为 Empty，当第一个消费者（主消费者）发送加入消费请求后，集群状态变更为 PreparingRebalance，并且会创建一个延迟操作等待后续更多的消费者加入消费组。
2）在指定的超时时间内如果没有新的消费者加入消费组或者等待的时间已经超过了最大值，则会强制完成延迟任务，返回加入消费组的响应给所有的消费者客户端，并将消费组的状态变更
为 CompletingRebalance，等待消费者发送同步消费组的请求。
3）消费者客户端在收到加入消费组响应后，主消费者会执行分区分配，并且会将分区的结果在发送同步消费组请求时同步给协调者，其他消费者也会发送同步消费组的请求，
区别是不会带分区分配结果，因为只有主消费者会可以执行分区分配。
4）协调者在收到消费者的同步消费组请求的处理逻辑也是不同的，通常情况下，普通消费者会先于主消费者发送 "同步消费组" 请求，因为主消费者需要执行分区分配，所以协调者如果
收到普通消费者的 "同步消费组" 请求，仅仅只是将回调函数保存到消费者元数据中，因为主消费者还没有发送  "同步消费组" 请求将分区分配结果带过来，还没有到返回  "同步消费组"
响应的时机。如果协调者收到了主消费者发送的  "同步消费组" 请求，除了保存回调函数，还需要将分区的分配结果持久化到内部主题中，完成持久化后，会执行消费组中每个消费者的回调函数，
将当前消费者的分区分配结果发送给消费者客户端，这样每个消费者就知道自己应该消费哪些分区，此时消费组的状态变更为 Stable。
在这个过程中，如果某个消费者因为一些原因发送 "同步消费组" 的请求延迟了，请求到达服务端协调者时消费组的状态已经变为 Stable 了，这样也没有关系，因为主消费者在完成持久化操作后，
会将各个消费者的分区分配结果都保存到了消费者元数据中了，此时的该消费者只需要从元数据中获取分区的分配结果并直接调用回调函数返回即可。


非主消费者发送 "加入消费组" 请求或 "同步消费组" 请求，但是消费组已经是 Stable 状态
    原因：服务端正常返回 "加入消费组" 的响应，但是可能由于网络或者消费者自身的原因没有收到这个响应。但是服务端消费组此刻的状态已经是 CompletingRebalance，它只需
等待消费者发送 "同步消费组" 的请求，协调者在收到主消费者发送的 "同步消费组" 的请求后，处理完成后，发送 "同步消费组" 的响应给各个消费者（不包括之前没有收到 "加入消费组"
请求的消费者，因为它没有发送 "同步消费组" 的请求，"同步消费组" 的请求依赖 "加入消费组" 的响应），然后消费组的状态变为 Stable。上面只针对非主消费者才能成立，对于主
消费者如果没有收到 "加入消费组" 响应，就无法执行分区分配的工作，由于协调者需要持久化分配结果并且返回 "同步消费组" 的请求只能由主消费的 "同步消费组" 的回调函数触发，
所以消费组的状态是无法变更为 Stable。
    处理：消费者在等待超时时间后没有收到 "加入消费组" 的响应，会重新发送 "加入消费组" 的请求，因为它认为没有发送成功，但是实际上该消费者的信息都已经成功保存到了消费组中，
此时再发送 "加入消费组" 的请求，因为本身的数据信息都没有发生变化，所以协调者会返回和之前相同的响应回去，如果这次消费者成功收到响应，会发送 "同步消费组" 的请求，协调者收
到请求后（此时的状态同样是 Stable），会将这个消费者分配的分区封装响应返回回去。协调者在处理 "同步消费组" 请求时，对于非主消费者只是保存回调函数，处理就结束了，并不返回
响应。对于主消费者，会将分配的结果持久化到内部主题中，然后调用各个消费者的回调函数，返回响应。


Kafka 消费者是如何避免提交未消费的偏移量（Kafka 为什么可以用拉取偏移量作为提交偏移量）
    在回答这个问题之前，首先要知道 Kafka 一次轮询所做的工作，定时提交偏移量这个工作就是在轮询中完成。在一次轮询中会判断提交任务是否超时，如果超时会立即提交偏移量。
之后发送拉取请求 -> 回调暂存拉取结果（此时没有更新偏移量）-> 拉取器调用获取记录集方法，更新订阅状态中的分区拉取偏移量，返回给客户端 -> 客户端处理返回的记录集。
整个流程可以看到，提交偏移量的操作在消费消息之后，一次轮询过程中，虽然在处理消息前偏移量已经更新，但是判断提交偏移量的操作是再此之前，如果要提交偏移量也必须等待下次
轮询，而在执行下次轮询时，本次的消息都已经处理完毕。所以下次提交的偏移量实际上是本次已经被消费完的消息，同理如果本次轮询也提交了偏移量，那提交的一定是上次更新的拉取
偏移量，并且这个位置以及之前的消息都已经被客户端消费了，所以才不会丢数据。



延迟生产
 request.required.acks = 0: 生产者不会等待服务端的任何应答。
 request.required.acks = 1: 服务端收到一个 ISR 集合里的副本完成数据同步的应答才返回给生产者（可能收到是主副本本身）。
 request.required.acks = -1: 服务端 ISR 集合中所有的备份副本都完成数据同步的应答后才返回应答给生产者。
这就意味着主副本在将数据写入后不能立即返回响应给生产者，可以采用阻塞的方式，但是这对服务端的性能有很大的影响。Kafka 针对这种需要延迟返回响应结果给
客户端的情况，专门会有一个延迟操作。

延迟加入：消费组所有的消费者都发送了加入组请求。
延迟心跳：没有限制，可以立即返回。
延迟生产：ISR 的所有副本都向主副本发送了应答。
延迟拉取：读取到足够数量的消息集。

延迟操作和外部事件的关系
1）服务端处理生产请求，追加消息集到主副本的本地日志后，会尝试完成延迟的拉取。服务端处理备份副本的拉取请求，向主副本的本地日志读取消息集后，会尝试完成延迟生产。

服务端处理的拉取请求可以来自消费者和备份副本。备份副本拉取主副本的消息，会尝试完成 延迟的生产，而消费者拉取主副本的消息时，并不会尝试完成 延迟生产。不过，生
产者追加消息到主副本的本地日志后，则可能会尝试完成消费者创建的 延迟拉取。P360

生产者追加消息创建延迟生产，它的限制条件是：所有备份副本发送应答给主副本。当备份副本发送应答给主副本，就会尝试完成延迟的生产请求。同样的，备份副本拉取消息创建延迟
拉取，它的限制条件是：拉取到足够的消息。当生产者追加消息到主副本后，表示有新消息，就会尝试完成延迟的拉取请求。




客户端创建消息时会为每条消息指定偏移量，每批消息的偏移量都是从 0 开始的，消息到达服务端后，会读日志文件中最近一条消息的偏移量，然后将这些相对偏移量转换为真正的消息偏移量，
存储到日志文件中。客户端消息带相对偏移量信息是为了在服务端修改相对偏移量信息时，原有的字节缓冲区可以保持不变。如果不带这个偏移量信息，服务端在给消息生成偏移量信息时，需要
将这个偏移量保存到消息的缓冲区里，这样就会改变原有消息的缓冲区的大小，会造成不可复用的问题。

Kafka 的分区分为主副本和备份副本。P309


todo huangran
再平衡过程中，消费者客户端是否会停止心跳？如果没有停止心跳，消费者客户端如何处理心跳的响应？
服务端拉取消息流程
服务端处理 OFFSET_FETCH 和 LIST_OFFSET 流程
为什么Kafka消费者不循环实现获取消息的逻辑，而是让消费者客户端来轮询
消息批数据格式
如何处理相对偏移量 ---> 绝对偏移量



协调者在处理完成消费者的 "加入消费组" 请求后，会返回响应给消费者，消费者在收到 "加入消费组" 的响应后，应该在会话时间内及时发送 "同步消费组" 的请求给协调者，
否则协调者就会认为消费者出现了故障。协调者在处理 "同步消费组" 请求时，有多个地方调用了 "完成并调度下一次心跳" 方法。
1）状态为 CompletingRebalance，在设置成员元数据的回调方法后调用。
2）状态为 Stable，在发送 "同步消费组" 响应给消费者后调用。
3）状态为 CompletingRebalance，在收到主消费这的 "同步消费组" 请求，给每个消费者发送 "同步消费组" 响应后调用。

协调者创建完 "延迟操作" 对象后，一个重要的步骤是：当延迟操作相关的外部事件发生时，就需要通过延迟缓存尝试完成延迟操作。对于 "延迟加入消费组"，外部事件是
消费者发送了 "加入消费组" 请求，对于 "延迟心跳"，外部事件是协调者与消费者之间有网络通信。不管是协调者处理消费者发送的请求还是，还是协调者发送响应给消费者,
协调者都会完成本次延迟心跳，并开始调度下一次心跳。



协调者接受不了消费者发送的心跳来监控消费者是否存活，如果消费者没有在指定的截止时间内发送心跳，协调者认为消费者失败，将其从消费组中移除，这样消费者就需要执行
再平衡操作。
另外，协调者在处理 "加入消费组" 和 "同步消费组" 过程中，为了保证参与加入组的消费者及时响应，也会用心跳来监控消费者成员还存活。



日志压缩是一种相对于基于时间和大小日志清理更加细粒度的日志保留策略，它基于清理点（cleanPoint），将所有旧日志分段的消息复制到新的日志分段上。
日志压缩前后，日志分段中每条消息的偏移量和写入时总是保持一致，但日志分段的偏移量不再是连续的，消息保持相对有序。
日志压缩后，消息的物理位置会发生变化，会生成新的的日志分段，日志分段中的每条消息的物理位置会重新按照文件来组织。

墓碑标记：一条带键的消息，内容为 null，表示这条删除的消息所在偏移量之前的所有消息都需要删除。


更新副本偏移量的场景：更新本地日志的偏移量，更新远程的备份副本偏移量
1）追加消息到主副本的本地日志、备份副本拉取消息写到自己的本地日志，都会更新日志的偏移量。
2）主副本所在的服务端处理备份副本的拉取请求，也会更新分区中备份副本对应的偏移量。

更新副本的最高水位的场景：更新主副本的最高水位，更新备份副本的最高水位
1）主副本的最高水位取决于 ISR 中所有副本的最小偏移量。最小值没有变化，最高水位也不会变化。
2）备份副本的最高水位取决于主副本的最高水位和它自己的偏移量，它会选择两者的最小值。

副本管理器会定时将所有分区的副本最高水位，刷写到复制点文件（replication-offset-checkpoint 检查点文件）
日志管理器会定时将所有分区的副本偏移量，刷写到恢复点文件（recovery-point-offset-checkpoint 检查点文件）

延迟拉取有两种：备份副本和消费者的拉取。备份副本或消费者的拉取请求都可以有多个分区，但是服务端完成延迟拉取操作并不需要等待所有分区都收集够最少字节数，
它只需要所有分区加起来的大小满足最少字节数，就可以返回拉取结果给备份副本或消费者。
与拉取请求相反，生产者如果有多个分区，服务端完成延迟生产操作，必须等待所有分区都被 ISR 所有副本同步后，才会返回生产结果给生产者。

延迟生产和延迟拉取
  外部事件通过指定分区尝试完成的延迟操作，如果延迟操作可以完成，其他分区中的延迟操作并不会被立即删除。这是因为分区作为延迟缓存的键，在服务端数量
会很多，如果一个个检查所有分区，再从延迟缓存中删除已经完成的延迟操作，速度就很慢。另外，如果采用这种方式，只要分区对应的延迟操作完成了一个，就要
立即检查所有分区，对服务端的性能影响也较大。所以，Kafka 的延迟缓存还有一个清理器，会负责定时地清理所有分区中已经完成的延迟操作。P388_P390



分区重新分配的逻辑
1）zk 节点的数据和控制器上下文

OAR: 分区的原始副本集合（分区重分配前）
RAR: 重新分配的副本集合（分区重分配后）
AR、ISR: 分区的当前副本集以及主副本保持同步的副本集

重新分配分区过程：
1）将 RAR 加入 AR，此时 AR = OAR + RAR
2）将 RAR 加入 ISR，此时 ISR = OAR + RAR
3）ISR 不变，但是主副本从 RAR 中选举
4）将 OAR 从 ISR 中移除，此时 ISR = RAR
5）将 OAR 从 AR 中移除，此时 AR = RAR
重新分配完分区后，分区的 AR 和 ISR 都等于 RAR。增加和修改 AR 和 ISR 的顺序是： 增加 AR -> 增加 ISR -> 减少 ISR -> 减少 AR。
因为副本在 AR 中，不一定在 ISR 中，而副本在 ISR 中，一般会在 AR 中。


控制器向代理节点发送的请求类型有三种：分区的主副本和 ISR 信息（LAI）、更新元数据（UpdateMetadata）、停止副本（StopReplicas）。代理节点处理这
三种请求，都会转交给副本管理器执行具体的逻辑。由于这三种请求都会更新分区的副本状态，因此这三个操作都会使用同一个对象锁进行同步。

整体流程：
1）控制器发送 LAI 请求给分区的所有代理节点，不同代理节点的分区会分别创建分区的主副本或备份副本。主副本会将分区从拉取管理器移除，备份副本会将分区加入
拉取管理器。
2）控制器发送 UpdateMetadata 请求给集群所有代理节点。
3）每个代理节点处理 UpdateMetadata 请求，都会更新元数据缓存。
4）客户端发送 Metadata 请求，从元数据缓存中获取主题的元数据。
5）客户端从主题元数据中找出分区的主副本，并和分区的主副本进行通信。


切换主副本 日志文件怎么处理的？
代理节点下线如果处理主分区的？


Kafka 创建主题过程
1）管理员创建主题，会在 /brokers/topics/下创建主题的子节点
2）更改主题监听器 调用 onNewTopicCreation() 向 zk 节点注册 更改分区监听器，并调用 onNewPartitionCreation() 创建分区节点。
3）控制器分别对分区、所有副本进行状态转换，最后分区和副本都转换为 上线 状态。
4）分区从 新建状态 到 上线状态，控制器会选举出主副本、创建分区状态的 zk 节点、并将最新的主副本和 ISR 信息（newLeaderAndIsr）更新到 zk 的分区
状态节点、更新上下文的分区缓存信息（partitionLeaderShipInfo）、发送 LAI 请求给分区的所有副本。

代理节点下线逻辑
1）将这些分区的状态从 上线状态 更改为 下线状态。
2）通过选举器（OfflinePartitionLeaderSelector）为分区选举新的主副本。
3）为分区选举新的主副本，会发送 LAI 请求给分区的所有存活的副本。
4）如果为分区选举出主副本，将分区状态从 下线状态 更改为 上线状态。
5）代理节点上所有副本的状态全部更改为 下线状态。

代理节点重新上线逻辑
重新上线逻辑和下线逻辑类似，都会更改分区状态机、副本状态机中受影响的分区和副本。代理节点下线时，控制器针对没有主副本的分区
（如分区只有一个副本且在下线节点上），通过选举新的主副本，分区状态从 下线 到 上线。对于有主副本的分区，仅转换状态即可。
如果没有成功选举出新的主副本，分区状态仍然是 下线。所以在代理节点上线时，还会触发一次分区的状态转换，将 下线状态 的分区转换为 上线状态。
对于副本状态机，代理节点下线时，副本状态为 下线；代理节点上线时，副本状态从 下线 到 上线。


日志管理器对日志进行管理，副本管理器对副本进行管理。日志管理器通过每个日志对象管理所有的日志分段。副本管理器也通过每个分区管理分区的所有的副本。

每个分区都只有一个主副本和多个备份副本，不同节点上的分区对象，它们的主副本对象都是一个（leaderReplicaIdOpt）变量。另外，分区对象还维护了所有的副本（AR）、同步的副本（ISR）。

副本对象
分区创建的副本分为本地副本和远程副本。节点编号和副本编号相同的副本叫做本地副本，编号不同的叫作远程副本。
● 本地副本有本地日志，远程副本没有日志。
● 创建本地副本时，会读取检查点文件中这个分区的初始最高水位。远程副本没有初始最高水位。

副本间数据同步
● 生产者客户端将消息集追加到分区的主副本。
● 消息集追加到主副本的本地日志，会更新日志的偏移量数据。
● 其他消息代理节点上的备份副本向主副本所在的代理节点同步数据。
● 主副本所在的副本管理器读取本地日志，并更新对应拉取的备份副本信息。
● 主副本所在的服务端将拉取的结果返回给发起拉取请求的备份副本。
● 备份副本接收到服务端返回的拉回结果，将消息集追加到本地日志，更新日志的偏移量元数据。
服务端处理备份副本的拉取请求，除了更新备份副本的偏移量元数据，还可能会
● 扩展分区的 ISR 集合。扩展 ISR 集必须满足下面三个条件：
	○ 这个备份副本之前不在分区的 ISR 集合中，如果已经在 ISR 集合中，就不需重复加入。
	○ 这个备份副本必须在分区的 AR 集合中，只有数据分区的副本，才会加入到 ISR 集合中。
	○ 这个备份副本的偏移量必须大于或等于主副本的最高水位，才会加入到 ISR 集合中。
● 更新主副本的最高水位
如果分区对应的主副本的最高水位有增加，会尝试完成 延迟生产 和 延迟拉取。
	○ 生产者客户端设置的应答值如果是 -1，则主副本必须等到 ISR 的所有备份副本都向主副本发送应答后，服务端才会返回响应结果给客户端。服务端完成 延迟生产 的外部触发事件就是备份副本发送应答，那么当备份副本向主副本发送拉取请求，服务端处理备份副本的拉取请求，就可能完成 延迟生产。
	○ 除了备份副本主动发送拉取请求可能会尝试完成 延迟生产，另外一种场景是：追加消息集到主副本的本地日志，如果 ISR 只有一个主副本，会立即增加主副本的 HW，并不需要等待其他备份副本发送应答。
主副本的 HW 增加了，就可以同时完成这两个延迟操作。
	○ 消费者最多只能消费到主副本的最高水位，如果消费者已经消费到最高水位，但是主副本的最高水位一直没有增加，服务端就不会返回拉取结果给消费者。而一旦主副本的 HW 增加了，就可以可能满足拉取到足够消息限制条件，服务端就可以返回拉取结果给消费者。
	○ 主副本等待 ISR 集合的所有备份副本都向它发送应答，在这之前，服务端不会返回生产响给生产者。主副本的 HW 会选择 ISR 集合中所有备份副本的最小偏移量值。服务端处理备份副本的拉取请求，会更新备份副本的偏移量，那么就有可能会增加主副本的 HW。一旦增加了主副本的 HW，表示 ISR 集合中所有副本一定都发送了应答，服务端就可以响应给生产者。
● 偏移量、最高水位、复制点
	○ 生产者往主副本写数据，主副本的 LEO 增加，初始时所有副本的 HW 都是 0。
	○ 备份副本拉取到数据，更新本地的 LEO。拉取响应带有主副本的 HW，但主副本的 HW 还是 0，备份副本的 HW 也为 0。
	○ 备份副本再次拉取数据，会更新主副本的 HW。主副本返回备份副本的拉取响应包含最新的 HW。
	○ 备份副本拉取到数据，更新本地 LEO，并且也会更新备份副本的 HW。
备份副本无论在服务端读取到多少条记录，服务端都会把读取到的所有记录返回给备份副本。服务端知道本次读取的记录条数，就可以在返回结果之前，更新备份副本对应的 LEO。备份副本在接收到拉取记录后，也会更新本地日志文件的 LEO，这样主副本记录的备份副本 LEO、备份副本自己记录的 LEO 是一致的数据。
更新副本的偏移量有下面两种场景：更新本地日志的偏移量、更新远程的备份偏移量。
● 追加消息到主副本的本地日志、备份副本拉取消息写到自己的本地日志，都会更新日志的偏移量。
● 主副本所在的服务端处理备份副本的拉取请求，也会更新分区中的备份副本对应的偏移量。
更新副本 HW 也有两种场景：更新主副本的最高水位、更新备份副本的最高水位。
● 主副本的 HW 取决于 ISR 中所有副本的最小偏移量。最小值没有变化，最高水位也不会变化。
● 备份副本的 HW 取决于主副本的 HW 和它自己的偏移量，它会选择这两者的最小值。

日志管理器会定时将所有分区的副本偏移量，刷写到恢复点文件（recovery-point-offset-checkpoint 检查点文件）。副本管理器也会定时将所有分区的副本最高水位，刷写到复制点文件（replication-offset-checkpoint 检查点文件）。
同一个分区在不同消息代理节点上，它们的本地副本都有偏移量和最高水位P372。主副本所在的节点会记录所有副本的偏移量，备份副本所在的节点只会记录它自己的偏移量，不会记录其他副本的偏移量。
对于消费者客户端而言，它最多只会读取到主副本的最高水位。但因为主副本可能会出现故障，所以备份副本也需要记录最高水位。当主副本出现故障时，备份副本成为主副本，它的最高水位
如果和之前的饿主副本的最高水平保持一致，消费者客户端就不会丢数据。





