This directory contains examples of client code that uses kafka.

To run the demo:

   1. Start Zookeeper and the Kafka server
   2. For unlimited sync-producer-consumer run, `run bin/java-producer-consumer-demo.sh sync`
   3. For unlimited async-producer-consumer run, `run bin/java-producer-consumer-demo.sh`
   4. For exactly once demo run, `run bin/exactly-once-demo.sh 6 3 50000`,
      this means we are starting 3 EOS instances with 6 topic partitions and 50000 pre-populated records.
   5. Some notes for exactly once demo:
      5.1. The Kafka server has to be on broker version 2.5 or higher.
      5.2. You could also use Intellij to run the example directly by configuring parameters as "Program arguments"

# 1. 配置环境，需要 gradle（6.6） 和 scala 环境，并配置环境变量，如果要启动服务，还需要配置 Zookeeper
# 2. 拉下代码到 idea，如果是官方源码，还需要先配置下 pom.xml 中的仓库地址，替换为阿里云仓库
# 3. 找到项目根目录下的 settings.grade 配置文件，添加到 gradle 工程，开始导入依赖
# 4. 依赖导入结束后，有些版本会有些包无法导入，需要手动执行执行 gradle wrapper, 生成 gradlew 执行文件，如果是 windows 环境，需要将根目录下
#   的 wrapper.gradle 文件中的最后一行移除 windows 批处理文件的配置给注释掉，不然在 windows 环境下根目录不会生成 gradlew.bat 批处理文件。
#   这是因为 Kafka 开发不是在 windows 环境，但是是可以在 windows 环境下编译的。
# 5. 在根目录下执行 ./gradlew assemble -x test 命令，重新编译下，编译成功后源码环境就搭建好了。
#    Note：1）因为 gradlew 没有配置环境变量，只能在项目根目录下执行。2）可以通过 ./gradlew tasks 查看所有可执行任务名称

# 如何保证消息的不丢失的方式
# 1. acks 设置为 -1，表示生产者发送完成消息后，只有 leader 节点和所有的 follower 节点都完成存储后，才给客户端发送响应。



● OP_ACCEPT 就绪条件：
当收到一个客户端连接请求时，该操作就绪。这是 ServerSocketChannel 上唯一有效的操作。
● OP_CONNECT 就绪条件：
	只有客户端 SocketChannel 会注册该事件，宕客户端调用 SocketChannel.connect() 时，该操作就绪。
● OP_READ 就绪条件：
该操作对客户端和服务端的 SocketChannel 都有效，当 OS 的读缓冲区中有数据可读时，该操作就绪。
● OP_WRITE 就绪条件：
	该操作对客户端和服务端的 SocketChannel 都有效，当 OS 的写缓冲区有空闲的空间时（大部分时候都有），该操作就绪。

OP_CONNECT
● 客户端调用 connect() 并注册 OP_CONNECT 事件后，连接操作就绪，但是连接就绪不代表连接成功。
● 在非阻塞模式下，如果是本地连接，连接会立刻建立，此时返回 true。其他场景下，连接是否完成需要通过 finishConnect() 方法判断。
● 在阻塞模式下，这个方法会一直阻塞直到连接建立或异常发生。

OP_ACCEPT
● 服务端见监听，并注册 OP_ACCEPT 事件后，就已经准备好接收客户端连接了。

OP_WRITE
● OP_WRITE 事件相对特殊，一般情况下，不应该注册 OP_WRITE 事件，OP_WRITE 就绪的条件为 OS 内核缓冲区有空闲空间（NIO 默认是水平触发，
OP_WRITE 事件是在 Socket 发送缓冲区中可用的字节数大于或等于其低水位标记 SO_SNDLOWAT 时发生），而写缓冲区绝大部分事件都是有空闲空间的，
所以当注册了该事件后，写操作一直就是就绪的，这样会导致Selector 处理线程会占用整个 CPU 资源。所以最佳实践是当确实有数据写入时再注册 OP_WRITE 事件，
并且在写完成以后马上取消注册。
●





select和poll都只提供了一个函数:select或者poll函数。
而epoll提供了三个函数，epoll_create,epoll_ctl和epoll_wait，epoll_create是创建一个epoll句柄；epoll_ctl是注册要监听的事件类型；epoll_wait则是等待事件的产生。

epoll除了提供select/poll那种IO事件的水平触发（Level Triggered）外，还提供了边缘触发（Edge Triggered），这就使得用户空间程序有可能缓存IO状态，减少epoll_wait/epoll_pwait的调用，提高应用程序效率。

水平触发(level-trggered)
只要文件描述符关联的读内核缓冲区非空，有数据可以读取，就一直发出可读信号进行通知，
当文件描述符关联的内核写缓冲区不满，有空间可以写入，就一直发出可写信号进行通知
LT模式支持阻塞和非阻塞两种方式。epoll默认的模式是LT。

边缘触发(edge-triggered)
当文件描述符关联的读内核缓冲区由空转化为非空的时候，则发出可读信号进行通知，
当文件描述符关联的内核写缓冲区由满转化为不满的时候，则发出可写信号进行通知
两者的区别在哪里呢？水平触发是只要读缓冲区有数据，就会一直触发可读信号，而边缘触发仅仅在空变为非空的时候通知一次，

LT(level triggered)是缺省的工作方式，并且同时支持block和no-block socket.在这种做法中，内核告诉你一个文件描述符是否就绪了，然后你可以对这个就绪的fd进行IO操作。
如果你不作任何操作，内核还是会继续通知你的，所以，这种模式编程出错误可能性要小一点。传统的select/poll都是这种模型的代表．

水平触发和边缘触发模式区别
读缓冲区刚开始是空的
读缓冲区写入2KB数据
水平触发和边缘触发模式此时都会发出可读信号
收到信号通知后，读取了1kb的数据，读缓冲区还剩余1KB数据
水平触发会再次进行通知，而边缘触发不会再进行通知
所以，边缘触发需要一次性的把缓冲区的数据读完为止，也就是一直读，直到读到EGAIN为止，EGAIN说明缓冲区已经空了，因为这一点，边缘触发需要设置文件句柄为非阻塞



epoll是一种Linux下高效的I/O多路复用机制，它支持两种不同的事件触发方式：水平触发和边缘触发。
在水平触发模式下，如果文件描述符上有数据可读或可写，epoll_wait()函数将立即返回该文件描述符，并且在下一次epoll_wait()调用之前将继续通知该文件描述符上的事件。
这意味着，如果你没有及时处理文件描述符上的事件，那么每次调用epoll_wait()都会返回相同的事件。


而在边缘触发模式下，只有当文件描述符状态发生变化时（例如，从无数据可读到有数据可读），epoll_wait()才会返回该文件描述符，并且只有在状态变化时才会再次通知该文件描述符上的事件。
这意味着你必须在epoll_wait()返回后立即处理该文件描述符上的所有事件，否则你可能会错过某些事件。




num.network.threads: 控制的 Processor 线程的个数，默认值为 3.
num.io.threads: 控制 Handler 线程的个数，默认值为 8.
queued.max_requests: 控制 RequestChannel 队列的容量，表示在网络线程停止读取新请求之前，可以排队等待I/O线程处理的最大请求个数，默认值 500.

增大 num.network.threads 能够增加处理网络io请求，但不读写具体数据，基本没有io等待。但如果过大，会带来线程切换的开销。
增大 queued.max.requests 能够缓存更多的请求，以撑过业务峰值。如果过大，会造成内存的浪费。
增大 num.io.threads 能提升线程处理能力，如果过大会代理线程切换开销影响处理能力。同时至少要等于硬盘的个数。


触发再平衡（re-balance）的条件：
  消费组成员列表发生变化，如新消费者加入或消费者退出消费组。
  订阅的主题的分区有变化。

消费者再平衡后重新分配分组，是如何保证各个消费者重新分组后消费消息的平滑过渡
  消费者会记录消费消息的偏移量，即消费进度。消费进度是维护在消费组里，当再平衡发生时，分配到新分区的消费者可以从消费组里读取该分区的消费进度，从而做到无缝迁移。



第一个消费者收到加入消费组的响应后执行分区分配，并将消费者的分配结果发送给协调者，协调者会发送同步消费组的响应给客户端，即分配给第一个消费者的分区
（因为此时只有一个消费者，所以是所有的分区），消费组的状态变为 Stable 状态。

当消费组状态为 Stable 后，第二个消费者加入消费组，此时消费组的状态会再变为 准备再平衡，并创建一个新的延迟操作对象。但是，此时这个延迟对象是无法完成的，因为
消费组中存在 awaitingJoinCallback = null 的消费者，即第一个消费者（因为消费组在发送加入消费组响应时，会将这个变量设置为 null），所以此时需要等待第一个
消费者重新加入消费组，延迟操作进入延迟缓存中。

如果第一个消费者在超时时间内重新加入了消费组，会通过延迟缓存再次执行刚刚创建的延迟操作，这次会满足条件，协调者会发送加入消费组响应给两个消费者。
如果在超时时间内第一个消费者没有发送加入消费组请求，第一个消费者的回调函数一直为 null，协调者在延迟操作超时后，强制执行延迟操作，但是这是只会
发送响应给第二个消费者。


正常情况下消费组状态变更过程
1）消费组初始状态为 Empty，当第一个消费者（主消费者）发送加入消费请求后，集群状态变更为 PreparingRebalance，并且会创建一个延迟操作等待后续更多的消费者加入消费组。
2）在指定的超时时间内如果没有新的消费者加入消费组或者等待的时间已经超过了最大值，则会强制完成延迟任务，返回加入消费组的响应给所有的消费者客户端，并将消费组的状态变更
为 CompletingRebalance，等待消费者发送同步消费组的请求。
3）消费者客户端在收到加入消费组响应后，主消费者会执行分区分配，并且会将分区的结果在发送同步消费组请求时同步给协调者，其他消费者也会发送同步消费组的请求，
区别是不会带分区分配结果，因为只有主消费者会可以执行分区分配。
4）协调者在收到消费者的同步消费组请求的处理逻辑也是不同的，通常情况下，普通消费者会先于主消费者发送 "同步消费组" 请求，因为主消费者需要执行分区分配，所以协调者如果
收到普通消费者的 "同步消费组" 请求，仅仅只是将回调函数保存到消费者元数据中，因为主消费者还没有发送  "同步消费组" 请求将分区分配结果带过来，还没有到返回  "同步消费组"
响应的时机。如果协调者收到了主消费者发送的  "同步消费组" 请求，除了保存回调函数，还需要将分区的分配结果持久化到内部主题中，完成持久化后，会执行消费组中每个消费者的回调函数，
将当前消费者的分区分配结果发送给消费者客户端，这样每个消费者就知道自己应该消费哪些分区，此时消费组的状态变更为 Stable。
在这个过程中，如果某个消费者因为一些原因发送 "同步消费组" 的请求延迟了，请求到达服务端协调者时消费组的状态已经变为 Stable 了，这样也没有关系，因为主消费者在完成持久化操作后，
会将各个消费者的分区分配结果都保存到了消费者元数据中了，此时的该消费者只需要从元数据中获取分区的分配结果并直接调用回调函数返回即可。


非主消费者发送 "加入消费组" 请求或 "同步消费组" 请求，但是消费组已经是 Stable 状态
    原因：服务端正常返回 "加入消费组" 的响应，但是可能由于网络或者消费者自身的原因没有收到这个响应。但是服务端消费组此刻的状态已经是 CompletingRebalance，它只需
等待消费者发送 "同步消费组" 的请求，协调者在收到主消费者发送的 "同步消费组" 的请求后，处理完成后，发送 "同步消费组" 的响应给各个消费者（不包括之前没有收到 "加入消费组"
请求的消费者，因为它没有发送 "同步消费组" 的请求，"同步消费组" 的请求依赖 "加入消费组" 的响应），然后消费组的状态变为 Stable。上面只针对非主消费者才能成立，对于主
消费者如果没有收到 "加入消费组" 响应，就无法执行分区分配的工作，由于协调者需要持久化分配结果并且返回 "同步消费组" 的请求只能由主消费的 "同步消费组" 的回调函数触发，
所以消费组的状态是无法变更为 Stable。
    处理：消费者在等待超时时间后没有收到 "加入消费组" 的响应，会重新发送 "加入消费组" 的请求，因为它认为没有发送成功，但是实际上该消费者的信息都已经成功保存到了消费组中，
此时再发送 "加入消费组" 的请求，因为本身的数据信息都没有发生变化，所以协调者会返回和之前相同的响应回去，如果这次消费者成功收到响应，会发送 "同步消费组" 的请求，协调者收
到请求后（此时的状态同样是 Stable），会将这个消费者分配的分区封装响应返回回去。协调者在处理 "同步消费组" 请求时，对于非主消费者只是保存回调函数，处理就结束了，并不返回
响应。对于主消费者，会将分配的结果持久化到内部主题中，然后调用各个消费者的回调函数，返回响应。


Kafka 消费者是如何避免提交未消费的偏移量（Kafka 为什么可以用拉取偏移量作为提交偏移量）
    在回答这个问题之前，首先要知道 Kafka 一次轮询所做的工作，定时提交偏移量这个工作就是在轮询中完成。在一次轮询中会判断提交任务是否超时，如果超时会立即提交偏移量。
之后发送拉取请求 -> 回调暂存拉取结果（此时没有更新偏移量）-> 拉取器调用获取记录集方法，更新订阅状态中的分区拉取偏移量，返回给客户端 -> 客户端处理返回的记录集。
整个流程可以看到，提交偏移量的操作在消费消息之后，一次轮询过程中，虽然在处理消息前偏移量已经更新，但是判断提交偏移量的操作是再此之前，如果要提交偏移量也必须等待下次
轮询，而在执行下次轮询时，本次的消息都已经处理完毕。所以下次提交的偏移量实际上是本次已经被消费完的消息，同理如果本次轮询也提交了偏移量，那提交的一定是上次更新的拉取
偏移量，并且这个位置以及之前的消息都已经被客户端消费了，所以才不会丢数据。


分区、副本、日志、日志分段

1）一个 topic 可以有多个分区，客户端以分区为最小处理单位生产或消费消息。以消费者为例，主题的分区越多，就可以启动越多的消费县城，消费者数量越多，消息的处理性能越好，延迟就越低。
2）Kafka 采用副本机制为一个分区备份多个分区，一个分区只有个主副本（Leader）和若干个备份副本（Follower）。主副本负责读写，备份副本负责向主副本拉取数据来同步，当主副本掉线后，
会从备份副本选举出一个新的主副本，继续为客户端提供读写服务。所以，每个副本会管理若干个主题的多个分区。
3）每个副本都对应一个日志，在将分区存储到底层的文件系统上，每个分区对应一个目录，分区目录下有多个日志分区（LogSegment），同一个目录下所有的日志分段都属于同一个分区。
4）每个日志分段在物理上是由一个数据文件和一个索引文件组成。数据文件存储真正的数据内容，索引文件存储的是数据文件的索引信息，方便随机读取分区时更快的访问数据文件。

// 补充一个全局关系图
分区到副本的映射关系可以认为是逻辑层，而副本和日志的关系则属于物理层。因为副本会真正存储在消息的代理节点上，所以会持有 Log 对象的引用，表示副本对应的日志文件。分区和
副本与外部交互的对象分别是客户端和消息代理节点。客户端访问分区，先获取分区的主副本，然后找到主副本所在的消息代理节点编号，最后从代理节点读写主副本对应的日志文件。

日志管理器和副本管理器
日志管理器和副本管理器的成员变量。副本管理器并不负责创建日志，它只是管理消息代理节点上的分区。所以，副本管理器嫁给你日志管理器这个全局的成员变量，传给了它所管理的每个分区。
副本管理器的每个分区会通过日志管理器，为每个副本创建对应的日志。

日志分段，由数据文件和索引文件组成。创建新的日志分段，会创建对应的数据文件和索引文件。
同一个分区的所有日志分段，所有消息的偏移量都是递增的。每个日志分段都有一个起始的绝对偏移量，日志分段之间的起始偏移量是递增的，一个日志分段里的偏移量也是递增的。

Kafka 的分区有主副本和备份副本，它们分布在不同的节点上。不同节点上的副本都有一个日志文件。因为每个副本在对应的消息代理节点上都有日志文件，所以每个副本也都有日志
的偏移量。日志和分区之间并没有直接的关联，日志和副本才有关联关系。分区通过管理主副本和备份副本，从而间接地管理所有副本中的日志。日志和副本是一一对应的，一个副本
对应一个日志。但实际上在 Kafka 分布式存储系统中，因为存在主副本和备份副本，副本如果不是本地的，就没有日志。总结下：副本有主副本和备份副本之分，日志则有本地和远程之分。


每个分区对应的日志对象管理了分区所有的日志分段
将消息追加到当前活动的日志分段，任何时刻，都只会有一个活动的日志分段。
每个日志文件对应一个数据文件和索引文件，消息会被追加到数据文件中。
操作系统底层数据的接口是文件通道，消息集提供一个 writeFullyTo()方法，参数是文件通道。
消息集（ByteBufferMessageSet）的 writeFullyTo() 方法，调用文件通道的 write() 方法，将底层包含消息内容的字节缓冲区（ByteBuffer）写到文件通道里。
字节缓冲区写到文件通道中，消息就持久化到日志分段对应的数据文件。

日志的偏移量元数据
1）nextOffsetMetadata：下一个偏移量元数据
2）logEndOffsetMetadata：日志结束偏移量元数据（不一定有值）
3）relativePositionInSegment: 消息在日志分段中的物理位置（不一定有值）

追加消息前，使用 nextOffsetMetadata 的消息偏移量，作为这批消息的起始偏移量。
如果滚动创建了日志分段，当前活动的日志会指向新创建的日志分段。
追加消息后，更新 nextOffsetMetadata 的消息偏移量，作为下一批消息的起始偏移量。

日志偏移量元数据是日志的一个重要特征。客户端对消息的读写操作，都会用到日志的偏移量信息。
写入消息集到日志，日志的 下一个偏移量（nextOffset）会作为消息集的 起始偏移量。
从日志读取消息时，不能超过日志 结束偏移量（logEndOffset）或最高水位（highWatermark）


校验消息集
1）是对客户端传递的消息集进行验证，确保每条消息的相对偏移量都是单调递增的。
2）删除消息集中无效的消息。如果大小一致，直接返回 message，否则会进行截断。

分配 offset
生产者客户端创建消息集时会为每条消息分配到一个相对偏移量（每个批次的消息集的起始偏移量都是从 0 开始），而 Kafka 存储消息时，会为每条消息都指定一个唯一的
偏移量。同一个分区的所有日志分段，它们的偏移量从 0 开始不断递增。
Kafka 在设计消息格式是保证了客户端生产的消息和服务端存储消息格式一致，这样服务端在存储消息时，以 nextOffsetMetadata 的偏移量作为起始偏移量，只需要直接修改字节
缓冲区每条消息的偏移量即可，其他内容保持不变，字节缓冲区的大小也不会发生变化。分配完成后还要更新 nextOffsetMetadata 的偏移量值。
消费线程（消费者或备份副本）会根据这个变量的最新值拉取消息，一旦变量值发生变化，消费线程就能拉取到新写入消息。

将更新了偏移量的消息集追加到当前活跃的日志分段中。

滚动创建日志分段
  为消息集分配偏移量后，日志会将消息追加到最新的日志分段。如果当前的日志分段放不下新追加的消息记录，日志会采用滚动方式创建一个新的日志分段，并将消息集追加到新创建
  的日志分段中。
  滚动创建日志分段的步骤：
   1）当前活动的日志分段指向旧的日志分段
   2）旧的日志分段空间不足，会创建新的日志分段
   3）当前活动的日志分段指向新的日志分段

  采用滚动方式创建日志分段的好处是，要根据时间、大小或偏移量3种策略删除日志的部分数据，实现起来比较容易。每个日志分段不仅有大小，也记录基准偏移量。如果要删除指定
  偏移量之前的数据，只需要选择满足条件的日志分段，并不需要获取分区的所有日志分段。

  日志分段创建的条件：
  1）当前数据文件大小 + 要写入数据大小超过数据文件的最大容量（1GB)，对应变量 log.segment.bytes
  2）每隔一段时间，自动创建一个新的数据文件， 对应变量 log.roll.hours
  3）其他一些条件，索引文件已满

写稀疏索引，默认是每写入 4096 个字节，写一个索引。索引是"消息绝对偏移量"到"消息在数据文件的物理位置"的映射。
1）Kafka 的索引是稀疏索引。
2）索引条目存储的是相对于基准偏移量的相对偏移量，不是消息的绝对偏移量。
3）索引条目的相对偏移量和物理位置各占用 4 个字节（绝对偏移量-基准偏移量），即一个索引条目占用 8 个字节。
4）偏移量是有序的，查询指定偏移量时，使用二分查找可以快速确定偏移量的位置。
5）指定偏移量如果在索引文件不存在，可以找到小于等于指定偏移量的最大偏移量。
6）稀疏索引可以通过内容映射的方式，将整个索引文件都存入内存，加快偏移量的查询。
补充一个索引图 P314 todo 这里图里的相对偏移量好像不太对

此外，在日志管理器（LogManager）初始化时会启动一个定时任务，在达到一定的时间间隔（默认是 Long.MAX_VALUE），也会触发一次刷新磁盘操作。

所以在默认情况下，Kafka 还是将刷新操作交由操作系统来完成，如果有需要，只要修改对应的参数即可。

其他的触发刷新日志的场景：
  1）创建新的 Segment，会立即刷新旧的日志分段
  2）日志中未刷新的消息数量超过 log.flush.interval.messages 配置的值（默认不刷新，由 OS 决定刷新时机）
  3）刷新日志定时任务（默认不刷新，由 OS 决定刷新时机）
 刷新日志方法的参数是日志的最新偏移量（logEndOffset），它要和日志中现有的检查点位置（recoveryPoint）比较，只有最新偏移量比检查点大，
 才需要刷新，完成刷新后，更新检查点的位置。

延迟生产
 request.required.acks = 0: 生产者不会等待服务端的任何应答。
 request.required.acks = 1: 服务端收到一个 ISR 集合里的副本完成数据同步的应答才返回给生产者（可能收到是主副本本身）。
 request.required.acks = -1: 服务端 ISR 集合中所有的备份副本都完成数据同步的应答后才返回应答给生产者。
这就意味着主副本在将数据写入后不能立即返回响应给生产者，可以采用阻塞的方式，但是这对服务端的性能有很大的影响。Kafka 针对这种需要延迟返回响应结果给
客户端的情况，专门会有一个延迟操作。

延迟加入：消费组所有的消费者都发送了加入组请求。
延迟心跳：没有限制，可以立即返回。
延迟生产：ISR 的所有副本都向主副本发送了应答。
延迟拉取：读取到足够数量的消息集。

延迟操作和外部事件的关系
1）服务端处理生产请求，追加消息集到主副本的本地日志后，会尝试完成延迟的拉取。服务端处理备份副本的拉取请求，向主副本的本地日志读取消息集后，会尝试完成延迟生产。

服务端处理的拉取请求可以来自消费者和备份副本。备份副本拉取主副本的消息，会尝试完成 延迟的生产，而消费者拉取主副本的消息时，并不会尝试完成 延迟生产。不过，生
产者追加消息到主副本的本地日志后，则可能会尝试完成消费者创建的 延迟拉取。P360

生产者追加消息创建延迟生产，它的限制条件是：所有备份副本发送应答给主副本。当备份副本发送应答给主副本，就会尝试完成延迟的生产请求。同样的，备份副本拉取消息创建延迟
拉取，它的限制条件是：拉取到足够的消息。当生产者追加消息到主副本后，表示有新消息，就会尝试完成延迟的拉取请求。


总结 P315
1）一个日志由多个日志分段组成，日志管理了所有的日志分段
2）日志用 segment 保存了每个日志分段的基准偏移量到日志分段的映射关系
3）日志分段的基准偏移量时分区级别的绝对偏移量
4）日志分段中的第一条消息的绝对偏移量也等于日志分段的基准偏移量
5）每个日志分段由一个数据文件和三个索引文件组成
6）日志分段的数据文件和索引文件的文件名称以基准偏移量命名
7）数据文件保存消息的格式是：消息的绝对偏移量、消息大小、消息内容
8）索引文件保存消息偏移量和消息在数据文件中的物理位置
9）索引文件中索引条目的存储的相对偏移量，即消息的绝对偏移量 - 基准偏移量
10）索引文件是保存在内存中的，将整个索引文件加载到内存中，加快文件的读取。





客户端创建消息时会为每条消息指定偏移量，每批消息的偏移量都是从 0 开始的，消息到达服务端后，会读日志文件中最近一条消息的偏移量，然后将这些相对偏移量转换为真正的消息偏移量，
存储到日志文件中。客户端消息带相对偏移量信息是为了在服务端修改相对偏移量信息时，原有的字节缓冲区可以保持不变。如果不带这个偏移量信息，服务端在给消息生成偏移量信息时，需要
将这个偏移量保存到消息的缓冲区里，这样就会改变原有消息的缓冲区的大小，会造成不可复用的问题。

Kafka 的分区分为主副本和备份副本。P309


todo huangran
再平衡过程中，消费者客户端是否会停止心跳？如果没有停止心跳，消费者客户端如何处理心跳的响应？
服务端拉取消息流程
服务端处理 OFFSET_FETCH 和 LIST_OFFSET 流程
为什么Kafka消费者不循环实现获取消息的逻辑，而是让消费者客户端来轮询
消息批数据格式
如何处理相对偏移量 ---> 绝对偏移量



协调者在处理完成消费者的 "加入消费组" 请求后，会返回响应给消费者，消费者在收到 "加入消费组" 的响应后，应该在会话时间内及时发送 "同步消费组" 的请求给协调者，
否则协调者就会认为消费者出现了故障。协调者在处理 "同步消费组" 请求时，有多个地方调用了 "完成并调度下一次心跳" 方法。
1）状态为 CompletingRebalance，在设置成员元数据的回调方法后调用。
2）状态为 Stable，在发送 "同步消费组" 响应给消费者后调用。
3）状态为 CompletingRebalance，在收到主消费这的 "同步消费组" 请求，给每个消费者发送 "同步消费组" 响应后调用。

协调者创建完 "延迟操作" 对象后，一个重要的步骤是：当延迟操作相关的外部事件发生时，就需要通过延迟缓存尝试完成延迟操作。对于 "延迟加入消费组"，外部事件是
消费者发送了 "加入消费组" 请求，对于 "延迟心跳"，外部事件是协调者与消费者之间有网络通信。不管是协调者处理消费者发送的请求还是，还是协调者发送响应给消费者,
协调者都会完成本次延迟心跳，并开始调度下一次心跳。



协调者接受不了消费者发送的心跳来监控消费者是否存活，如果消费者没有在指定的截止时间内发送心跳，协调者认为消费者失败，将其从消费组中移除，这样消费者就需要执行
再平衡操作。
另外，协调者在处理 "加入消费组" 和 "同步消费组" 过程中，为了保证参与加入组的消费者及时响应，也会用心跳来监控消费者成员还存活。



日志压缩是一种相对于基于时间和大小日志清理更加细粒度的日志保留策略，它基于清理点（cleanPoint），将所有旧日志分段的消息复制到新的日志分段上。
日志压缩前后，日志分段中每条消息的偏移量和写入时总是保持一致，但日志分段的偏移量不再是连续的，消息保持相对有序。
日志压缩后，消息的物理位置会发生变化，会生成新的的日志分段，日志分段中的每条消息的物理位置会重新按照文件来组织。

墓碑标记：一条带键的消息，内容为 null，表示这条删除的消息所在偏移量之前的所有消息都需要删除。


更新副本偏移量的场景：更新本地日志的偏移量，更新远程的备份副本偏移量
1）追加消息到主副本的本地日志、备份副本拉取消息写到自己的本地日志，都会更新日志的偏移量。
2）主副本所在的服务端处理备份副本的拉取请求，也会更新分区中备份副本对应的偏移量。

更新副本的最高水位的场景：更新主副本的最高水位，更新备份副本的最高水位
1）主副本的最高水位取决于 ISR 中所有副本的最小偏移量。最小值没有变化，最高水位也不会变化。
2）备份副本的最高水位取决于主副本的最高水位和它自己的偏移量，它会选择两者的最小值。

副本管理器会定时将所有分区的副本最高水位，刷写到复制点文件（replication-offset-checkpoint 检查点文件）
日志管理器会定时将所有分区的副本偏移量，刷写到恢复点文件（recovery-point-offset-checkpoint 检查点文件）

延迟拉取有两种：备份副本和消费者的拉取。备份副本或消费者的拉取请求都可以有多个分区，但是服务端完成延迟拉取操作并不需要等待所有分区都收集够最少字节数，
它只需要所有分区加起来的大小满足最少字节数，就可以返回拉取结果给备份副本或消费者。
与拉取请求相反，生产者如果有多个分区，服务端完成延迟生产操作，必须等待所有分区都被 ISR 所有副本同步后，才会返回生产结果给生产者。

延迟生产和延迟拉取
  外部事件通过指定分区尝试完成的延迟操作，如果延迟操作可以完成，其他分区中的延迟操作并不会被立即删除。这是因为分区作为延迟缓存的键，在服务端数量
会很多，如果一个个检查所有分区，再从延迟缓存中删除已经完成的延迟操作，速度就很慢。另外，如果采用这种方式，只要分区对应的延迟操作完成了一个，就要
立即检查所有分区，对服务端的性能影响也较大。所以，Kafka 的延迟缓存还有一个清理器，会负责定时地清理所有分区中已经完成的延迟操作。P388_P390



分区重新分配的逻辑
1）zk 节点的数据和控制器上下文

OAR: 分区的原始副本集合（分区重分配前）
RAR: 重新分配的副本集合（分区重分配后）
AR、ISR: 分区的当前副本集以及主副本保持同步的副本集

重新分配分区过程：
1）将 RAR 加入 AR，此时 AR = OAR + RAR
2）将 RAR 加入 ISR，此时 ISR = OAR + RAR
3）ISR 不变，但是主副本从 RAR 中选举
4）将 OAR 从 ISR 中移除，此时 ISR = RAR
5）将 OAR 从 AR 中移除，此时 AR = RAR
重新分配完分区后，分区的 AR 和 ISR 都等于 RAR。增加和修改 AR 和 ISR 的顺序是： 增加 AR -> 增加 ISR -> 减少 ISR -> 减少 AR。
因为副本在 AR 中，不一定在 ISR 中，而副本在 ISR 中，一般会在 AR 中。


控制器向代理节点发送的请求类型有三种：分区的主副本和 ISR 信息（LAI）、更新元数据（UpdateMetadata）、停止副本（StopReplicas）。代理节点处理这
三种请求，都会转交给副本管理器执行具体的逻辑。由于这三种请求都会更新分区的副本状态，因此这三个操作都会使用同一个对象锁进行同步。

整体流程：
1）控制器发送 LAI 请求给分区的所有代理节点，不同代理节点的分区会分别创建分区的主副本或备份副本。主副本会将分区从拉取管理器移除，备份副本会将分区加入
拉取管理器。
2）控制器发送 UpdateMetadata 请求给集群所有代理节点。
3）每个代理节点处理 UpdateMetadata 请求，都会更新元数据缓存。
4）客户端发送 Metadata 请求，从元数据缓存中获取主题的元数据。
5）客户端从主题元数据中找出分区的主副本，并和分区的主副本进行通信。


切换主副本 日志文件怎么处理的？
代理节点下线如果处理主分区的？


Kafka 创建主题过程
1）管理员创建主题，会在 /brokers/topics/下创建主题的子节点
2）更改主题监听器 调用 onNewTopicCreation() 向 zk 节点注册 更改分区监听器，并调用 onNewPartitionCreation() 创建分区节点。
3）控制器分别对分区、所有副本进行状态转换，最后分区和副本都转换为 上线 状态。
4）分区从 新建状态 到 上线状态，控制器会选举出主副本、创建分区状态的 zk 节点、并将最新的主副本和 ISR 信息（newLeaderAndIsr）更新到 zk 的分区
状态节点、更新上下文的分区缓存信息（partitionLeaderShipInfo）、发送 LAI 请求给分区的所有副本。

代理节点下线逻辑
1）将这些分区的状态从 上线状态 更改为 下线状态。
2）通过选举器（OfflinePartitionLeaderSelector）为分区选举新的主副本。
3）为分区选举新的主副本，会发送 LAI 请求给分区的所有存活的副本。
4）如果为分区选举出主副本，将分区状态从 下线状态 更改为 上线状态。
5）代理节点上所有副本的状态全部更改为 下线状态。

代理节点重新上线逻辑
重新上线逻辑和下线逻辑类似，都会更改分区状态机、副本状态机中受影响的分区和副本。代理节点下线时，控制器针对没有主副本的分区
（如分区只有一个副本且在下线节点上），通过选举新的主副本，分区状态从 下线 到 上线。对于有主副本的分区，仅转换状态即可。
如果没有成功选举出新的主副本，分区状态仍然是 下线。所以在代理节点上线时，还会触发一次分区的状态转换，将 下线状态 的分区转换为 上线状态。
对于副本状态机，代理节点下线时，副本状态为 下线；代理节点上线时，副本状态从 下线 到 上线。


日志管理器对日志进行管理，副本管理器对副本进行管理。日志管理器通过每个日志对象管理所有的日志分段。副本管理器也通过每个分区管理分区的所有的副本。

每个分区都只有一个主副本和多个备份副本，不同节点上的分区对象，它们的主副本对象都是一个（leaderReplicaIdOpt）变量。另外，分区对象还维护了所有的副本（AR）、同步的副本（ISR）。

副本对象
分区创建的副本分为本地副本和远程副本。节点编号和副本编号相同的副本叫做本地副本，编号不同的叫作远程副本。
● 本地副本有本地日志，远程副本没有日志。
● 创建本地副本时，会读取检查点文件中这个分区的初始最高水位。远程副本没有初始最高水位。

副本间数据同步
● 生产者客户端将消息集追加到分区的主副本。
● 消息集追加到主副本的本地日志，会更新日志的偏移量数据。
● 其他消息代理节点上的备份副本向主副本所在的代理节点同步数据。
● 主副本所在的副本管理器读取本地日志，并更新对应拉取的备份副本信息。
● 主副本所在的服务端将拉取的结果返回给发起拉取请求的备份副本。
● 备份副本接收到服务端返回的拉回结果，将消息集追加到本地日志，更新日志的偏移量元数据。
服务端处理备份副本的拉取请求，除了更新备份副本的偏移量元数据，还可能会
● 扩展分区的 ISR 集合。扩展 ISR 集必须满足下面三个条件：
	○ 这个备份副本之前不在分区的 ISR 集合中，如果已经在 ISR 集合中，就不需重复加入。
	○ 这个备份副本必须在分区的 AR 集合中，只有数据分区的副本，才会加入到 ISR 集合中。
	○ 这个备份副本的偏移量必须大于或等于主副本的最高水位，才会加入到 ISR 集合中。
● 更新主副本的最高水位
如果分区对应的主副本的最高水位有增加，会尝试完成 延迟生产 和 延迟拉取。
	○ 生产者客户端设置的应答值如果是 -1，则主副本必须等到 ISR 的所有备份副本都向主副本发送应答后，服务端才会返回响应结果给客户端。服务端完成 延迟生产 的外部触发事件就是备份副本发送应答，那么当备份副本向主副本发送拉取请求，服务端处理备份副本的拉取请求，就可能完成 延迟生产。
	○ 除了备份副本主动发送拉取请求可能会尝试完成 延迟生产，另外一种场景是：追加消息集到主副本的本地日志，如果 ISR 只有一个主副本，会立即增加主副本的 HW，并不需要等待其他备份副本发送应答。
主副本的 HW 增加了，就可以同时完成这两个延迟操作。
	○ 消费者最多只能消费到主副本的最高水位，如果消费者已经消费到最高水位，但是主副本的最高水位一直没有增加，服务端就不会返回拉取结果给消费者。而一旦主副本的 HW 增加了，就可以可能满足拉取到足够消息限制条件，服务端就可以返回拉取结果给消费者。
	○ 主副本等待 ISR 集合的所有备份副本都向它发送应答，在这之前，服务端不会返回生产响给生产者。主副本的 HW 会选择 ISR 集合中所有备份副本的最小偏移量值。服务端处理备份副本的拉取请求，会更新备份副本的偏移量，那么就有可能会增加主副本的 HW。一旦增加了主副本的 HW，表示 ISR 集合中所有副本一定都发送了应答，服务端就可以响应给生产者。
● 偏移量、最高水位、复制点
	○ 生产者往主副本写数据，主副本的 LEO 增加，初始时所有副本的 HW 都是 0。
	○ 备份副本拉取到数据，更新本地的 LEO。拉取响应带有主副本的 HW，但主副本的 HW 还是 0，备份副本的 HW 也为 0。
	○ 备份副本再次拉取数据，会更新主副本的 HW。主副本返回备份副本的拉取响应包含最新的 HW。
	○ 备份副本拉取到数据，更新本地 LEO，并且也会更新备份副本的 HW。
备份副本无论在服务端读取到多少条记录，服务端都会把读取到的所有记录返回给备份副本。服务端知道本次读取的记录条数，就可以在返回结果之前，更新备份副本对应的 LEO。备份副本在接收到拉取记录后，也会更新本地日志文件的 LEO，这样主副本记录的备份副本 LEO、备份副本自己记录的 LEO 是一致的数据。
更新副本的偏移量有下面两种场景：更新本地日志的偏移量、更新远程的备份偏移量。
● 追加消息到主副本的本地日志、备份副本拉取消息写到自己的本地日志，都会更新日志的偏移量。
● 主副本所在的服务端处理备份副本的拉取请求，也会更新分区中的备份副本对应的偏移量。
更新副本 HW 也有两种场景：更新主副本的最高水位、更新备份副本的最高水位。
● 主副本的 HW 取决于 ISR 中所有副本的最小偏移量。最小值没有变化，最高水位也不会变化。
● 备份副本的 HW 取决于主副本的 HW 和它自己的偏移量，它会选择这两者的最小值。

日志管理器会定时将所有分区的副本偏移量，刷写到恢复点文件（recovery-point-offset-checkpoint 检查点文件）。副本管理器也会定时将所有分区的副本最高水位，刷写到复制点文件（replication-offset-checkpoint 检查点文件）。
同一个分区在不同消息代理节点上，它们的本地副本都有偏移量和最高水位P372。主副本所在的节点会记录所有副本的偏移量，备份副本所在的节点只会记录它自己的偏移量，不会记录其他副本的偏移量。
对于消费者客户端而言，它最多只会读取到主副本的最高水位。但因为主副本可能会出现故障，所以备份副本也需要记录最高水位。当主副本出现故障时，备份副本成为主副本，它的最高水位
如果和之前的饿主副本的最高水平保持一致，消费者客户端就不会丢数据。
