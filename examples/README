This directory contains examples of client code that uses kafka.

To run the demo:

   1. Start Zookeeper and the Kafka server
   2. For unlimited sync-producer-consumer run, `run bin/java-producer-consumer-demo.sh sync`
   3. For unlimited async-producer-consumer run, `run bin/java-producer-consumer-demo.sh`
   4. For exactly once demo run, `run bin/exactly-once-demo.sh 6 3 50000`,
      this means we are starting 3 EOS instances with 6 topic partitions and 50000 pre-populated records.
   5. Some notes for exactly once demo:
      5.1. The Kafka server has to be on broker version 2.5 or higher.
      5.2. You could also use Intellij to run the example directly by configuring parameters as "Program arguments"

# 1. 配置环境，需要 gradle（6.6） 和 scala 环境，并配置环境变量，如果要启动服务，还需要配置 Zookeeper
# 2. 拉下代码到 idea，如果是官方源码，还需要先配置下仓库地址，替换为阿里云仓库
# 3. 找到项目根目录下的 settings.grade 配置文件，添加到 gradle 工程，开始导入依赖
# 4. 依赖导入结束后，有些版本会有些包无法导入，需要手动执行执行 gradle wrapper, 生成 gradlew 执行文件，如果是 windows 环境，需要将根目录下
#   的 wrapper.gradle 文件中的最后一行移除 windows 批处理文件的配置给注释掉，不然在 windows 环境下根目录不会生成 gradlew.bat 批处理文件。
#   这是因为 Kafka 开发不是在 windows 环境，但是是可以在 windows 环境下编译的。
# 5. 在根目录下执行 ./gradlew assemble -x test 命令，重新编译下，编译成功后源码环境就搭建好了。
#    Note：1）因为 gradlew 没有配置环境变量，只能在项目根目录下执行。2）可以通过 ./gradlew tasks 查看所有可执行任务名称

# 如何保证消息的不丢失的方式
# 1. acks 设置为 -1，表示生产者发送完成消息后，只有 leader 节点和所有的 follower 节点都完成存储后，才给客户端发送响应。



● OP_ACCEPT 就绪条件：
当收到一个客户端连接请求时，该操作就绪。这是 ServerSocketChannel 上唯一有效的操作。
● OP_CONNECT 就绪条件：
	只有客户端 SocketChannel 会注册该事件，宕客户端调用 SocketChannel.connect() 时，该操作就绪。
● OP_READ 就绪条件：
该操作对客户端和服务端的 SocketChannel 都有效，当 OS 的读缓冲区中有数据可读时，该操作就绪。
● OP_WRITE 就绪条件：
	该操作对客户端和服务端的 SocketChannel 都有效，当 OS 的写缓冲区有空闲的空间时（大部分时候都有），该操作就绪。

OP_CONNECT
● 客户端调用 connect() 并注册 OP_CONNECT 事件后，连接操作就绪，但是连接就绪不代表连接成功。
● 在非阻塞模式下，如果是本地连接，连接会立刻建立，此时返回 true。其他场景下，连接是否完成需要通过 finishConnect() 方法判断。
● 在阻塞模式下，这个方法会一直阻塞直到连接建立或异常发生。

OP_ACCEPT
● 服务端见监听，并注册 OP_ACCEPT 事件后，就已经准备好接收客户端连接了。

OP_WRITE
● OP_WRITE 事件相对特殊，一般情况下，不应该注册 OP_WRITE 事件，OP_WRITE 就绪的条件为 OS 内核缓冲区有空闲空间（NIO 默认是水平触发，
OP_WRITE 事件是在 Socket 发送缓冲区中可用的字节数大于或等于其低水位标记 SO_SNDLOWAT 时发生），而写缓冲区绝大部分事件都是有空闲空间的，
所以当注册了该事件后，写操作一直就是就绪的，这样会导致Selector 处理线程会占用整个 CPU 资源。所以最佳实践是当确实有数据写入时再注册 OP_WRITE 事件，
并且在写完成以后马上取消注册。
●





select和poll都只提供了一个函数:select或者poll函数。
而epoll提供了三个函数，epoll_create,epoll_ctl和epoll_wait，epoll_create是创建一个epoll句柄；epoll_ctl是注册要监听的事件类型；epoll_wait则是等待事件的产生。

epoll除了提供select/poll那种IO事件的水平触发（Level Triggered）外，还提供了边缘触发（Edge Triggered），这就使得用户空间程序有可能缓存IO状态，减少epoll_wait/epoll_pwait的调用，提高应用程序效率。

水平触发(level-trggered)
只要文件描述符关联的读内核缓冲区非空，有数据可以读取，就一直发出可读信号进行通知，
当文件描述符关联的内核写缓冲区不满，有空间可以写入，就一直发出可写信号进行通知
LT模式支持阻塞和非阻塞两种方式。epoll默认的模式是LT。

边缘触发(edge-triggered)
当文件描述符关联的读内核缓冲区由空转化为非空的时候，则发出可读信号进行通知，
当文件描述符关联的内核写缓冲区由满转化为不满的时候，则发出可写信号进行通知
两者的区别在哪里呢？水平触发是只要读缓冲区有数据，就会一直触发可读信号，而边缘触发仅仅在空变为非空的时候通知一次，

LT(level triggered)是缺省的工作方式，并且同时支持block和no-block socket.在这种做法中，内核告诉你一个文件描述符是否就绪了，然后你可以对这个就绪的fd进行IO操作。
如果你不作任何操作，内核还是会继续通知你的，所以，这种模式编程出错误可能性要小一点。传统的select/poll都是这种模型的代表．

水平触发和边缘触发模式区别
读缓冲区刚开始是空的
读缓冲区写入2KB数据
水平触发和边缘触发模式此时都会发出可读信号
收到信号通知后，读取了1kb的数据，读缓冲区还剩余1KB数据
水平触发会再次进行通知，而边缘触发不会再进行通知
所以，边缘触发需要一次性的把缓冲区的数据读完为止，也就是一直读，直到读到EGAIN为止，EGAIN说明缓冲区已经空了，因为这一点，边缘触发需要设置文件句柄为非阻塞



epoll是一种Linux下高效的I/O多路复用机制，它支持两种不同的事件触发方式：水平触发和边缘触发。
在水平触发模式下，如果文件描述符上有数据可读或可写，epoll_wait()函数将立即返回该文件描述符，并且在下一次epoll_wait()调用之前将继续通知该文件描述符上的事件。
这意味着，如果你没有及时处理文件描述符上的事件，那么每次调用epoll_wait()都会返回相同的事件。


而在边缘触发模式下，只有当文件描述符状态发生变化时（例如，从无数据可读到有数据可读），epoll_wait()才会返回该文件描述符，并且只有在状态变化时才会再次通知该文件描述符上的事件。
这意味着你必须在epoll_wait()返回后立即处理该文件描述符上的所有事件，否则你可能会错过某些事件。




num.network.threads: 控制的 Processor 线程的个数，默认值为 3.
num.io.threads: 控制 Handler 线程的个数，默认值为 8.
queued.max_requests: 控制 RequestChannel 队列的容量，表示在网络线程停止读取新请求之前，可以排队等待I/O线程处理的最大请求个数，默认值 500.

增大 num.network.threads 能够增加处理网络io请求，但不读写具体数据，基本没有io等待。但如果过大，会带来线程切换的开销。
增大 queued.max.requests 能够缓存更多的请求，以撑过业务峰值。如果过大，会造成内存的浪费。
增大 num.io.threads 能提升线程处理能力，如果过大会代理线程切换开销影响处理能力。同时至少要等于硬盘的个数。


触发再平衡（re-balance）的条件：
  消费组成员列表发生变化，如新消费者加入或消费者退出消费组。
  订阅的主题的分区有变化。

消费者再平衡后重新分配分组，是如何保证各个消费者重新分组后消费消息的平滑过渡
  消费者会记录消费消息的偏移量，即消费进度。消费进度是维护在消费组里，当再平衡发生时，分配到新分区的消费者可以从消费组里读取该分区的消费进度，从而做到无缝迁移。



第一个消费者收到加入消费组的响应后执行分区分配，并将消费者的分配结果发送给协调者，协调者会发送同步消费组的响应给客户端，即分配给第一个消费者的分区
（因为此时只有一个消费者，所以是所有的分区），消费组的状态变为 Stable 状态。

当消费组状态为 Stable 后，第二个消费者加入消费组，此时消费组的状态会再变为 准备再平衡，并创建一个新的延迟操作对象。但是，此时这个延迟对象是无法完成的，因为
消费组中存在 awaitingJoinCallback = null 的消费者，即第一个消费者（因为消费组在发送加入消费组响应时，会将这个变量设置为 null），所以此时需要等待第一个
消费者重新加入消费组，延迟操作进入延迟缓存中。

如果第一个消费者在超时时间内重新加入了消费组，会通过延迟缓存再次执行刚刚创建的延迟操作，这次会满足条件，协调者会发送加入消费组响应给两个消费者。
如果在超时时间内第一个消费者没有发送加入消费组请求，第一个消费者的回调函数一直为 null，协调者在延迟操作超时后，强制执行延迟操作，但是这是只会
发送响应给第二个消费者。


正常情况下消费组状态变更过程
1）消费组初始状态为 Empty，当第一个消费者（主消费者）发送加入消费请求后，集群状态变更为 PreparingRebalance，并且会创建一个延迟操作等待后续更多的消费者加入消费组。
2）在指定的超时时间内如果没有新的消费者加入消费组或者等待的时间已经超过了最大值，则会强制完成延迟任务，返回加入消费组的响应给所有的消费者客户端，并将消费组的状态变更
为 CompletingRebalance，等待消费者发送同步消费组的请求。
3）消费者客户端在收到加入消费组响应后，主消费者会执行分区分配，并且会将分区的结果在发送同步消费组请求时同步给协调者，其他消费者也会发送同步消费组的请求，
区别是不会带分区分配结果，因为只有主消费者会可以执行分区分配。
4）协调者在收到消费者的同步消费组请求的处理逻辑也是不同的，通常情况下，普通消费者会先于主消费者发送 "同步消费组" 请求，因为主消费者需要执行分区分配，所以协调者如果
收到普通消费者的 "同步消费组" 请求，仅仅只是将回调函数保存到消费者元数据中，因为主消费者还没有发送  "同步消费组" 请求将分区分配结果带过来，还没有到返回  "同步消费组"
响应的时机。如果协调者收到了主消费者发送的  "同步消费组" 请求，除了保存回调函数，还需要将分区的分配结果持久化到内部主题中，完成持久化后，会执行消费组中每个消费者的回调函数，
将当前消费者的分区分配结果发送给消费者客户端，这样每个消费者就知道自己应该消费哪些分区，此时消费组的状态变更为 Stable。
在这个过程中，如果某个消费者因为一些原因发送 "同步消费组" 的请求延迟了，请求到达服务端协调者时消费组的状态已经变为 Stable 了，这样也没有关系，因为主消费者在完成持久化操作后，
会将各个消费者的分区分配结果都保存到了消费者元数据中了，此时的该消费者只需要从元数据中获取分区的分配结果并直接调用回调函数返回即可。


非主消费者发送 "加入消费组" 请求或 "同步消费组" 请求，但是消费组已经是 Stable 状态
    原因：服务端正常返回 "加入消费组" 的响应，但是可能由于网络或者消费者自身的原因没有收到这个响应。但是服务端消费组此刻的状态已经是 CompletingRebalance，它只需
等待消费者发送 "同步消费组" 的请求，协调者在收到主消费者发送的 "同步消费组" 的请求后，处理完成后，发送 "同步消费组" 的响应给各个消费者（不包括之前没有收到 "加入消费组"
请求的消费者，因为它没有发送 "同步消费组" 的请求，"同步消费组" 的请求依赖 "加入消费组" 的响应），然后消费组的状态变为 Stable。上面只针对非主消费者才能成立，对于主
消费者如果没有收到 "加入消费组" 响应，就无法执行分区分配的工作，由于协调者需要持久化分配结果并且返回 "同步消费组" 的请求只能由主消费的 "同步消费组" 的回调函数触发，
所以消费组的状态是无法变更为 Stable。
    处理：消费者在等待超时时间后没有收到 "加入消费组" 的响应，会重新发送 "加入消费组" 的请求，因为它认为没有发送成功，但是实际上该消费者的信息都已经成功保存到了消费组中，
此时再发送 "加入消费组" 的请求，因为本身的数据信息都没有发生变化，所以协调者会返回和之前相同的响应回去，如果这次消费者成功收到响应，会发送 "同步消费组" 的请求，协调者收
到请求后（此时的状态同样是 Stable），会将这个消费者分配的分区封装响应返回回去。协调者在处理 "同步消费组" 请求时，对于非主消费者只是保存回调函数，处理就结束了，并不返回
响应。对于主消费者，会将分配的结果持久化到内部主题中，然后调用各个消费者的回调函数，返回响应。


todo huangran
再平衡过程中，消费者客户端是否会停止心跳？如果没有停止心跳，消费者客户端如何处理心跳的响应？



协调者在处理完成消费者的 "加入消费组" 请求后，会返回响应给消费者，消费者在收到 "加入消费组" 的响应后，应该在会话时间内及时发送 "同步消费组" 的请求给协调者，
否则协调者就会认为消费者出现了故障。协调者在处理 "同步消费组" 请求时，有多个地方调用了 "完成并调度下一次心跳" 方法。
1）状态为 CompletingRebalance，在设置成员元数据的回调方法后调用。
2）状态为 Stable，在发送 "同步消费组" 响应给消费者后调用。
3）状态为 CompletingRebalance，在收到主消费这的 "同步消费组" 请求，给每个消费者发送 "同步消费组" 响应后调用。

协调者创建完 "延迟操作" 对象后，一个重要的步骤是：当延迟操作相关的外部事件发生时，就需要通过延迟缓存尝试完成延迟操作。对于 "延迟加入消费组"，外部事件是
消费者发送了 "加入消费组" 请求，对于 "延迟心跳"，外部事件是协调者与消费者之间有网络通信。不管是协调者处理消费者发送的请求还是，还是协调者发送响应给消费者,
协调者都会完成本次延迟心跳，并开始调度下一次心跳。



协调者接受不了消费者发送的心跳来监控消费者是否存活，如果消费者没有在指定的截止时间内发送心跳，协调者认为消费者失败，将其从消费组中移除，这样消费者就需要执行
再平衡操作。
另外，协调者在处理 "加入消费组" 和 "同步消费组" 过程中，为了保证参与加入组的消费者及时响应，也会用心跳来监控消费者成员还存活。
